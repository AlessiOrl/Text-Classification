{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "bonus1.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "ZOxkT8uRYrrN",
    "uxQPaFFZYrEr",
    "lkvcpZA7mLm5",
    "Lj41VW9VpamL",
    "BlzXfuhvhF0L",
    "REtpu0AnhSq5",
    "ukKiTN7Uhj-_"
   ],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#Init Colab"
   ],
   "metadata": {
    "id": "ZOxkT8uRYrrN"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pJbYXou6chZf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "01fb35fc-fefa-4b47-f02a-acb045c0ba33"
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 19 14:21:28 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 511.65       Driver Version: 511.65       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:09:00.0  On |                  N/A |\n",
      "| 30%   43C    P0    33W / 170W |    990MiB /  8192MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       956    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      1372    C+G   ...Container\\nvcontainer.exe    N/A      |\n",
      "|    0   N/A  N/A      1472    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      2252    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A      3196    C+G                                   N/A      |\n",
      "|    0   N/A  N/A      6052    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      8652    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9212    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     11492    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     11652    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     11952    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
      "|    0   N/A  N/A     12792    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     12800    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     13468    C+G                                   N/A      |\n",
      "|    0   N/A  N/A     14644    C+G   ...150.39\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     14788    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15288    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16092    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     16292    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     16704    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     18236    C+G   ... iCUE 4 Software\\iCUE.exe    N/A      |\n",
      "|    0   N/A  N/A     19560    C+G   ...gvanyjgm\\app\\WhatsApp.exe    N/A      |\n",
      "|    0   N/A  N/A     19744    C+G   ...4vj0pshhgkwm\\Telegram.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Init"
   ],
   "metadata": {
    "id": "uxQPaFFZYrEr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## for data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## for processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "\n",
    "## for model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "##Other\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FTiqzWuHj-qU",
    "outputId": "542989b7-390a-4053-9800-7631494f16c8"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\orlan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\orlan\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nj3_4TC6hF0C",
    "outputId": "22c5e8bf-9f78-4a01-b3c1-7dbade811c87"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gensim_vocab_name = \"glove-wiki-gigaword-50\""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "47hUHnFghF0D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded_word_vocab = gensim.downloader.load(gensim_vocab_name)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cJbTIvEshF0D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def seed_all(seed: int = 42):\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "id": "YOw5Na5dlm3o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "seed_all()"
   ],
   "metadata": {
    "id": "H0UNjqW7lrEW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1b542be6-a42f-410f-a5e6-0319d0a1a6ff"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Using Seed :  42  ]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "temp_path = \".\"\n",
    "out_path = os.path.join(temp_path, \"out\")\n",
    "data_path = os.path.join(temp_path, \"data\")\n",
    "model_p = temp_path\n",
    "train_path = os.path.join(data_path, \"train.jsonl\")\n",
    "dev_path = os.path.join(data_path, \"dev.jsonl\")\n",
    "test_path = os.path.join(data_path, \"test.jsonl\")"
   ],
   "metadata": {
    "id": "TzTsem6OltUT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# WordClassificationDataset"
   ],
   "metadata": {
    "id": "lkvcpZA7mLm5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class WCDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_file: str,\n",
    "                 vocab,\n",
    "                 pos_vocab,\n",
    "                 sw_filter=False,\n",
    "                 lemming=True,\n",
    "                 lowercase=True,\n",
    "                 max_len=100,\n",
    "                 device=\"cpu\",\n",
    "                 test=False):\n",
    "\n",
    "        self.input_file = input_file\n",
    "        self.lowercase = lowercase\n",
    "        self.lemming = lemming\n",
    "        self.sw_filter = sw_filter\n",
    "        self.device = device\n",
    "        self.encoded_data = []\n",
    "        self.class_label = self.get_class_labels()\n",
    "        self.vocab = vocab\n",
    "        self.test = test\n",
    "        self.pos_vocab = pos_vocab\n",
    "        self.data = self.load_jsonl(self.input_file)\n",
    "        self.init_structures(self.data, max_len)\n",
    "\n",
    "    def init_structures(self, sentences, max_len) -> None:\n",
    "        for d in sentences:\n",
    "            data = self.getwords(d)\n",
    "            words_idx, pos_idx = self.index_words(data, max_len)\n",
    "\n",
    "\n",
    "            if not self.test:\n",
    "                label_idx = self.class_label.get(d['label'], 15)\n",
    "            else:\n",
    "                label_idx = 15\n",
    "            if len(words_idx) == 0: continue\n",
    "\n",
    "            words_idx = torch.tensor(words_idx)\n",
    "            words_idx = nn.ConstantPad1d((0, max(0, max_len - words_idx.shape[0])), self.vocab[\"<PAD>\"])(words_idx)\n",
    "\n",
    "\n",
    "            pos_idx = torch.tensor(pos_idx)\n",
    "            pos_idx = nn.ConstantPad1d((0, max(0, max_len - pos_idx.shape[0])), self.pos_vocab[\"<PAD>\"])(pos_idx)\n",
    "\n",
    "            self.encoded_data.append(({'word_indx': words_idx, 'pos_indx': pos_idx}, label_idx, d['id']))\n",
    "\n",
    "    def getwords(self, data):\n",
    "        \"\"\"\n",
    "          Returns the words (cleaned and filtered) of the sentences.\n",
    "          id, lemma, pos, label\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        s = data['text'].lower()\n",
    "\n",
    "        s = re.sub('[:;!@#$()\\-&<>/,.]', '', s)\n",
    "\n",
    "        s = nltk.word_tokenize(s)\n",
    "        cachedStopWords = stopwords.words(\"english\")\n",
    "        s = nltk.pos_tag(s, tagset='universal')\n",
    "        out_s = []\n",
    "\n",
    "        for word, pos in s:\n",
    "            if self.sw_filter and word in cachedStopWords: continue\n",
    "\n",
    "            if self.lemming: out_s.append((lemmatizer.lemmatize(word, pos=self.get_wordnet_pos(pos)), pos))\n",
    "\n",
    "            else: out_s.append((word, pos))\n",
    "        return {'words': out_s}\n",
    "\n",
    "    def index_words(self, data: Dict, max_len: int):\n",
    "        idxs = []\n",
    "        pos_idxs = []\n",
    "        words = data['words']\n",
    "        for i, (word, pos) in enumerate(words):\n",
    "            if len(idxs) == max_len: break\n",
    "            if word in self.vocab.keys():\n",
    "                idxs.append(self.vocab[word])\n",
    "            else:\n",
    "                idxs.append(self.vocab[\"<UNK>\"])\n",
    "            pos_idxs.append(self.pos_vocab.get(pos, self.pos_vocab.get('<UNK>')))\n",
    "        return idxs, pos_idxs\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_class(index):\n",
    "        for key, val in WCDataset.get_class_labels.items():\n",
    "            if val == index:\n",
    "                return key\n",
    "\n",
    "    @staticmethod\n",
    "    def get_class_labels()-> Dict:\n",
    "        return {\"business\": 0, \"crime\": 1, \"culture/arts\": 2, \"education\": 3, \"entertainment\": 4,\n",
    "                \"environment\": 5, \"food/drink\": 6, \"home/living\": 7, \"media\": 8, \"politics\": 9, \"religion\": 10,\n",
    "                \"sci/tech\": 11, \"sports\": 12, \"wellness\": 13, \"world\": 14}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_class_from_index(x: int):\n",
    "        for c, ci in WCDataset.get_class_labels().items():\n",
    "            if WCDataset.get_class_labels()[c] == x: return c\n",
    "        return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_jsonl(input_path):\n",
    "        data = []\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "        print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "        if treebank_tag.startswith('ADJ'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('VERB'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('ADV'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def getData(self):\n",
    "        return self.encoded_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "\n",
    "        words = torch.stack([e[0][\"word_indx\"] for e in data])\n",
    "        posses = torch.stack([e[0][\"pos_indx\"] for e in data])\n",
    "\n",
    "        y = torch.tensor([e[1] for e in data])\n",
    "\n",
    "        ids = torch.tensor([e[2] for e in data])\n",
    "        return (words, posses), y, ids"
   ],
   "metadata": {
    "id": "O9S6dQyPxvUj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_pos_embs(emb_size):\n",
    "    pos_list = [\"ADJ\", \"ADP\", \"PUNCT\", \"ADV\", \"AUX\", \"SYM\", \"INTJ\", \"CCONJ\", \"X\", \"NOUN\", \"DET\", \"PROPN\", \"NUM\", \"VERB\",\n",
    "                \"PART\", \"PRON\", \"SCONJ\", \"<UNK>\", \"<PAD>\"]\n",
    "    pos_embs = []\n",
    "    pos_index = dict()\n",
    "    for seed, vocab in enumerate(pos_list):\n",
    "        np.random.seed(seed)\n",
    "        pos_embs.append(np.random.rand(emb_size))\n",
    "        pos_index[vocab] = seed\n",
    "    return np.array(pos_embs, dtype=float), pos_index"
   ],
   "metadata": {
    "id": "5O2CBc6wc47h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WCParams:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    lemming = True\n",
    "    sw_filter = True\n",
    "    max_sentence_len = 75\n",
    "\n",
    "    word_vocab_name = gensim_vocab_name\n",
    "    word_vocab = loaded_word_vocab\n",
    "\n",
    "    pos_vocab_embs, pos_vocab_index = create_pos_embs(10)\n",
    "\n",
    "    word_vocab_size = len(word_vocab)\n",
    "    word_embedding_dim = word_vocab.vector_size\n",
    "    pos_embedding_dim = len(pos_vocab_embs[0])\n",
    "\n",
    "    word_vocab[\"<UNK>\"] = np.zeros(word_embedding_dim)\n",
    "    word_vocab[\"<PAD>\"] = np.zeros(word_embedding_dim)\n",
    "\n",
    "    num_classes = 15\n",
    "    hidden_dims = [word_embedding_dim, num_classes]\n",
    "\n",
    "    epochs = 20\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3 #default 0.001\n",
    "    weight_decay = 1e-4\n",
    "    dropout = 0.15\n",
    "\n",
    "    csv = \",\".join(\n",
    "        [\"device\", \"lemming\", \"sw_filter\", \"max_sentence_len\", \"word_vocab\", \"word_vocab_size\", \"word_embedding_dim\", \"pos_embedding_dim\",\n",
    "         \"num_classes\", \"hidden_dims\", \"epochs\", \"batch_size\", \"learning_rate\", \"weight_decay\", \"dropout\"])\n",
    "    csv += \"\\n\"\n",
    "    csv += \",\".join(\n",
    "        [str(device), str(lemming), str(sw_filter), str(max_sentence_len), str(word_vocab_name), str(word_vocab_size), str(word_embedding_dim),\n",
    "         str(pos_embedding_dim), str(num_classes), str(hidden_dims), str(epochs), str(batch_size), str(learning_rate),\n",
    "         str(weight_decay), str(dropout)])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "2y5GfrKVhF0J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "id": "Lj41VW9VpamL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WCModel(nn.Module):\n",
    "    def __init__(self, params: WCParams):\n",
    "        super(WCModel, self).__init__()\n",
    "        self.hidden_dim = params.hidden_dims\n",
    "        self.params = params\n",
    "\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.params.word_vocab.vectors),\n",
    "                                                           freeze=False)\n",
    "        self.pos_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(self.params.pos_vocab_embs),\n",
    "                                                           freeze=False)\n",
    "        self.layers = []\n",
    "        for i in range(len(self.hidden_dim) - 1):\n",
    "            self.layers += [\n",
    "                nn.BatchNorm1d(self.hidden_dim[i]),\n",
    "                nn.Linear(self.hidden_dim[i], self.hidden_dim[i + 1]),\n",
    "                nn.Dropout(params.dropout),\n",
    "                nn.LeakyReLU(),\n",
    "            ]\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "        self.sequential = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_out = self.word_embedding(x[0])\n",
    "\n",
    "        #words mean\n",
    "        sentence_emb = torch.mean(word_out, 1)\n",
    "\n",
    "        #sequential\n",
    "        out = self.sequential(sentence_emb)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x):\n",
    "        self.eval()\n",
    "\n",
    "        logits = self.forward(x)\n",
    "\n",
    "        preds = logits.argmax(1)\n",
    "\n",
    "        return preds"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FFkbvO5KhF0K"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Def Train-Eval functions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "BlzXfuhvhF0L"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, criterion, device):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y, sentence_id) in train_dataloader:\n",
    "        x = (x[0].to(device), x[1].to(device))\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        top = y_pred.argmax(1)\n",
    "        correct += torch.sum(top == y).item()\n",
    "        total  += y.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "    return epoch_loss / len(train_dataloader),  correct / total\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y, sentence_id) in iterator:\n",
    "            x = (x[0].to(device), x[1].to(device))\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            top = y_pred.argmax(1)\n",
    "            correct += torch.sum(top == y).item()\n",
    "            total  += y.size(0)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), correct/total"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zfWdnJ65hF0L"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Init Params Dataloaders and Model"
   ],
   "metadata": {
    "id": "REtpu0AnhSq5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Init Model\n",
    "params = WCParams()"
   ],
   "metadata": {
    "id": "sxi8-5ckseEe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = WCDataset(train_path, params.word_vocab.key_to_index, params.pos_vocab_index, lemming=params.lemming,\n",
    "                          sw_filter=params.sw_filter, max_len=params.max_sentence_len, device=params.device)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=params.batch_size, collate_fn=train_dataset.collate_fn, shuffle=True)"
   ],
   "metadata": {
    "id": "Fjn-ZnSUxWqL",
    "outputId": "c3a2f143-f0e9-40a4-9e4f-94b432d77e44"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 186282 records from .\\data\\train.jsonl\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6844 records from .\\data\\dev.jsonl\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = WCDataset(dev_path, params.word_vocab.key_to_index, params.pos_vocab_index, lemming=params.lemming,\n",
    "                        sw_filter=params.sw_filter, max_len=params.max_sentence_len, device=params.device)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=params.batch_size, collate_fn=dev_dataset.collate_fn)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Bmj31AWthF0M",
    "outputId": "54a4cd1b-f424-492e-a7e8-69d4a9b313e8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = WCModel(params).to(params.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zF_1gdUqhF0N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train\n",
    "\n"
   ],
   "metadata": {
    "id": "eLCHLZTRpiti"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "best_valid_loss = float('inf')\n",
    "losses = {\"train\": [], \"val\": []}\n",
    "accuracies = {\"train\": [], \"val\": []}\n",
    "\n",
    "for epoch in range(params.epochs):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion, params.device)\n",
    "    valid_loss, valid_acc = evaluate(model, dev_dataloader, criterion, params.device)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), os.path.join(temp_path,'model.ckpt'))\n",
    "\n",
    "    losses[\"train\"].append(train_loss)\n",
    "    losses[\"val\"].append(valid_loss)\n",
    "\n",
    "    accuracies[\"train\"].append(train_acc)\n",
    "    accuracies[\"val\"].append(valid_acc)\n",
    "\n",
    "    print(\" #### EPOCH {} ####\".format(str(epoch + 1)))\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Err: {100 - (train_acc * 100):.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Err: {100 - (valid_acc * 100):.2f}%')"
   ],
   "metadata": {
    "id": "7BgZNSSkiz-5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot train graph"
   ],
   "metadata": {
    "id": "ukKiTN7Uhj-_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(losses[\"train\"], label=\"train\")\n",
    "plt.plot(losses[\"val\"], label=\"val\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "uAQWefknhF0O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(accuracies[\"train\"],label=\"train\" )\n",
    "\n",
    "plt.plot(accuracies[\"val\"], label=\"val\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QXunAlp3hF0O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save parameters"
   ],
   "metadata": {
    "id": "d_RCu7n3ho0c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "out_file = os.path.join(temp_path, \"model_params.csv\")\n",
    "with open(out_file, \"w\") as f:\n",
    "    f.write(params.csv)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "wLwMGEezhF0O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction Section"
   ],
   "metadata": {
    "collapsed": false,
    "id": "moyi1gj5hF0P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p= \".\"\n",
    "prediction_path= os.path.join(model_p, \"predictions\")\n",
    "model.load_state_dict(torch.load(os.path.join(model_p,'model.ckpt')))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dOgwXdj8hF0P",
    "outputId": "f244a212-1c11-4777-88b3-403c6ab8a762"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Init test Dataloader"
   ],
   "metadata": {
    "id": "pfvgFnO6h1oD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6849 records from .\\data\\test.jsonl\n"
     ]
    }
   ],
   "source": [
    "test_dataset = WCDataset(test_path, params.word_vocab.key_to_index, params.pos_vocab_index, lemming=params.lemming,\n",
    "                         sw_filter=params.sw_filter, max_len=params.max_sentence_len, device=params.device, test=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=params.batch_size, collate_fn=dev_dataset.collate_fn)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7UelANEghF0P",
    "outputId": "7702a127-ed63-4441-e211-a126c77bba2a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate *predictions_dev.tsv* and *predictions_test.tsv*"
   ],
   "metadata": {
    "id": "4pD9WY16h_pZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict_path = os.path.join(os.path.join(prediction_path, \"predictions_dev.tsv\"))\n",
    "with open(predict_path, \"w\") as f:\n",
    "    txt = \"\"\n",
    "    with torch.no_grad():\n",
    "        for (x, _, sentence_id) in dev_dataloader:\n",
    "            x = (x[0].to(params.device), x[1].to(params.device))\n",
    "            y_pred = model.predict(x)\n",
    "            for index, sid in enumerate(sentence_id):\n",
    "                txt += \"{}\\t{}\\n\".format(str(int(sid)),WCDataset.get_class_from_index(y_pred[index]))\n",
    "    f.write(txt)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "1ACAg9R-hF0Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predict_path = os.path.join(os.path.join(prediction_path, \"predictions_test.tsv\"))\n",
    "with open(predict_path, \"w\") as f:\n",
    "    txt = \"\"\n",
    "    with torch.no_grad():\n",
    "        for (x, _, sentence_id) in test_dataloader:\n",
    "            x = (x[0].to(params.device), x[1].to(params.device))\n",
    "            y_pred = model.predict(x)\n",
    "            for index, sid in enumerate(sentence_id):\n",
    "                txt += \"{}\\t{}\\n\".format(str(int(sid)),WCDataset.get_class_from_index(y_pred[index]))\n",
    "    f.write(txt)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "y2bONy9khF0Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confirm evaluation with scorer.py"
   ],
   "metadata": {
    "id": "9_EAsqDSiKOY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'err_rate': '23.55'}\n"
     ]
    }
   ],
   "source": [
    "! python3 scorer.py --prediction_file ./predictions/predict_dev.tsv --gold_file ./gold/gold_dev.tsv"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IMrhOr5whF0Q",
    "outputId": "52768182-c618-43d6-e93d-425e78620e58"
   }
  }
 ]
}